{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMmz+Oq/0CvGvlkvN0z8Gkx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ru2zi/Deep-Learning-Approaches-to-RUL-Prediction-of-Lithium-ion-Batteries/blob/main/RUL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mn3FkOSdo5xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4FnWxoUoxyw"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/RUL/5. 배터리 RUL 예측/Battery_RUL_data.csv')"
      ],
      "metadata": {
        "id": "XhL6_VreqGRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "UmLJUriMYM3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "43_v9_dRYPOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "w3iGZ1O1Zs8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Selecting only float64 data columns\n",
        "float_cols = df.select_dtypes(include=['float64']).columns\n",
        "\n",
        "# Splitting columns into groups of 3\n",
        "col_sets = [float_cols[n:n+3] for n in range(0, len(float_cols), 3)]\n",
        "\n",
        "for cols in col_sets:\n",
        "    # Change rows and cols to make it horizontal\n",
        "    fig = make_subplots(rows=len(cols), cols=1, subplot_titles=cols)\n",
        "    for idx, col in enumerate(cols, start=1):\n",
        "        hist_fig = px.histogram(df, x=col)\n",
        "        for trace in hist_fig.data:\n",
        "            fig.add_trace(trace, row=idx, col=1)  # Change the row and col\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "veBWHV2ZqshE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Selecting only float64 data columns\n",
        "float_cols = df.select_dtypes(include=['float64']).columns\n",
        "\n",
        "for col in float_cols:\n",
        "    fig = px.histogram(df, x=col, histnorm='percent', cumulative=True)\n",
        "    fig.update_layout(title=f'Cumulative Distribution Function for {col}')\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "9_MUCw_bZB7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이상치\n",
        "### Z-score:\n",
        "- 정규분포 가정: Z-score는 데이터가 근사적으로 정규분포를 따르는 경우 더 효과적입니다. Z-score는 데이터 포인트의 값이 평균에서 얼마나 떨어져 있는지를 나타내며, 표준편차 단위로 측정됩니다. <br>\n",
        "- 표준화된 척도: Z-score는 데이터의 절대적인 값을 사용하지 않기 때문에 다양한 데이터 집합 간에 이상치를 비교하거나 식별하는 데 유용합니다.<br>\n",
        "- 정확한 위치 식별: Z-score는 각 데이터 포인트에 대해 개별적으로 계산되므로 이상치의 정확한 위치를 식별하는 데 도움이 됩니다.<br>\n",
        "\n",
        "### IQR:\n",
        "- 비정규분포 데이터: IQR은 데이터가 정규분포를 따르지 않을 때 또는 분포에 대해 알 수 없을 때 유용합니다.<br>\n",
        "- 로버스트성: IQR은 중앙값과 사분위수를 기반으로 하므로, 극단값에 덜 민감하며, 이상치의 영향을 적게 받습니다."
      ],
      "metadata": {
        "id": "T5nxRiDDc6bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of specified columns for outlier removal\n",
        "specified_cols = ['Discharge Time (s)', 'Decrement 3.6-3.4V (s)', 'Time at 4.15V (s)', 'Time constant current (s)', 'Charging time (s)']\n"
      ],
      "metadata": {
        "id": "UG7HDJipbSVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking normality using Shapiro-Wilk test\n",
        "normality_results = {}\n",
        "\n",
        "for col in specified_cols:\n",
        "    stat, p = shapiro(df[col])\n",
        "    normality_results[col] = p > 0.05  # True if normal, False otherwise\n",
        "\n",
        "normality_results\n"
      ],
      "metadata": {
        "id": "x7cwy4HbeBJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평균과 표준편차를 사용하여 z-score를 계산하고, z-score가 일정 임계값을 초과하는 값을 이상치로 간주\n",
        "\n",
        "# Define a function to detect outliers based on Z-score\n",
        "def zscore_outliers(data, column, threshold=3):\n",
        "    z_scores = np.abs(zscore(data[column]))\n",
        "    return np.where(z_scores > threshold)[0]\n",
        "\n",
        "# Applying Z-score method\n",
        "zscore_outliers_dict = {}\n",
        "for col in specified_cols:\n",
        "    zscore_outliers_dict[col] = zscore_outliers(df, col)\n",
        "\n",
        "# Combine outliers from both methods (Grubbs' test and Z-score method)\n",
        "combined_outliers_z = set()\n",
        "for col in specified_cols:\n",
        "    combined_outliers_z.update(outliers[col])\n",
        "    combined_outliers_z.update(zscore_outliers_dict[col])\n",
        "\n",
        "# Remove the outliers\n",
        "df_cleaned_z = df.drop(index=combined_outliers_z).reset_index(drop=True)\n",
        "\n",
        "df_cleaned_z\n"
      ],
      "metadata": {
        "id": "nRPelaLIbLZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Selecting only float64 data columns\n",
        "float_cols = df_cleaned_z.select_dtypes(include=['float64']).columns\n",
        "\n",
        "# Splitting columns into groups of 3\n",
        "col_sets = [float_cols[n:n+3] for n in range(0, len(float_cols), 3)]\n",
        "\n",
        "for cols in col_sets:\n",
        "    # Change rows and cols to make it horizontal\n",
        "    fig = make_subplots(rows=len(cols), cols=1, subplot_titles=cols)\n",
        "    for idx, col in enumerate(cols, start=1):\n",
        "        hist_fig = px.histogram(df_cleaned_z, x=col)\n",
        "        for trace in hist_fig.data:\n",
        "            fig.add_trace(trace, row=idx, col=1)  # Change the row and col\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "ipW02ZW1cErt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to remove outliers based on IQR\n",
        "def remove_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Filter out the outliers\n",
        "    filtered = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
        "\n",
        "    return filtered\n",
        "\n",
        "# Apply the outlier removal for each specified column\n",
        "for col in specified_cols:\n",
        "    df_IQR = remove_outliers_iqr(df, col)\n",
        "\n",
        "df_IQR.reset_index(drop=True, inplace=True)  # Resetting the index after removing rows\n",
        "df_IQR\n"
      ],
      "metadata": {
        "id": "pNwRdngsc5vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리"
      ],
      "metadata": {
        "id": "XGGlpEJvgd5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ws4TbvHTgwvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_IQR.drop('RUL', axis=1)\n",
        "y = df_IQR['RUL']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "OubWA3cxgo0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)  #train꺼로 transform한걸로 fit 중요!!!!\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "XBeOcmitgfdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델"
      ],
      "metadata": {
        "id": "sbJCn7CUhHsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[EMD-CNN-LSTM](http://210.101.116.16/xml/html/%EC%A0%84%EB%A0%A5%EC%A0%84%EC%9E%90%ED%95%99%ED%9A%8C/%EA%B5%AD%EB%AC%B8/27_1/7.%EC%9E%84%EC%A0%9C%EC%98%81/%EC%9E%84%EC%A0%9C%EC%98%81.html)"
      ],
      "metadata": {
        "id": "TbyE82xNhA93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EMD: 시계열 데이터를 IMF로 분해합니다. <br>\n",
        "CNN: 각 IMF를 CNN을 통해 처리하고 특징을 추출합니다.<br>\n",
        "LSTM: CNN에서 추출된 특징을 LSTM에 공급하여 시계열 예측을 수행합니다."
      ],
      "metadata": {
        "id": "Q02yHfdYhOU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install EMD-signal"
      ],
      "metadata": {
        "id": "wtbEEn5DhW8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyEMD import EMD\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. EMD Decomposition (for demonstration, we'll just decompose one of the columns)\n",
        "selected_column = X_train_scaled[:, 0]  # Selecting the first column for demonstration\n",
        "emd = EMD()\n",
        "IMFs = emd(selected_column)\n",
        "\n",
        "# Reshaping IMFs for CNN\n",
        "IMFs_reshaped = IMFs.T.reshape(IMFs.shape[1], IMFs.shape[0], 1)\n",
        "\n",
        "# 2. Create the EMD-CNN-LSTM model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(IMFs_reshaped.shape[1], IMFs_reshaped.shape[2])))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Splitting IMFs for training and validation\n",
        "train_size = int(len(IMFs_reshaped) * 0.8)\n",
        "train_IMFs, test_IMFs = IMFs_reshaped[:train_size], IMFs_reshaped[train_size:]\n",
        "train_y, test_y = y_train[:train_size], y_train[train_size:]\n",
        "\n",
        "# Setting up callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# Train the model with callbacks\n",
        "history = model.fit(train_IMFs, train_y, epochs=100, batch_size=16, validation_data=(test_IMFs, test_y),\n",
        "                    verbose=2, callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Visualizing training and validation loss\n",
        "loss_values = history.history['loss']\n",
        "val_loss_values = history.history['val_loss']\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "# Plotting the loss values\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=epochs, y=loss_values, mode='lines', name='Training Loss'))\n",
        "fig.add_trace(go.Scatter(x=epochs, y=val_loss_values, mode='lines', name='Validation Loss'))\n",
        "fig.update_layout(title='Training and Validation Loss', xaxis_title='Epoch', yaxis_title='Loss')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ibcYD0pQhQ9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 성능 확인"
      ],
      "metadata": {
        "id": "UvfRK4ZJjiMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "# Visualizing the predicted vs actual RUL values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Actual RUL values\n",
        "fig.add_trace(go.Scatter(y=test_y, mode='lines', name='Actual RUL'))\n",
        "\n",
        "# Predicted RUL values\n",
        "fig.add_trace(go.Scatter(y=y_pred.flatten(), mode='lines', name='Predicted RUL'))\n",
        "\n",
        "fig.update_layout(title='Actual vs Predicted RUL', xaxis_title='Index', yaxis_title='RUL Value')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "1bHRnj1-i78U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Mean Absolute Percentage Error (MAPE)\n",
        "mape = 100 * np.mean(np.abs((test_y - y_pred.flatten()) / test_y))\n",
        "mape"
      ],
      "metadata": {
        "id": "BWfkchkvjGAm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}